This folder contains all the code I used to generate my network for estimating gaze, as well as the weights used in the network. This program uses the frames dataset to train the network and perform evaluation on the test set. To obtain the test and train files, you can run genTxtFiles which creates/updates the train.txt and test.txt files. Note that you will need to record the number of training and testing images that written to the terminal after running genTxtFiles.py. These are parameters at the top of oldRun.py and run.py which need to be updated to the accurate number of testing and training images. As for the program itself, oldRun.py is an older iteration of the network and system design. Originally, I planned to use only the x and y pixel estimates from clm tracker to map to the correct location on the screen. The idea behind this is that the linear regression function does not accurately capture the change in eye pixel location to the change in screen pixel location. Instead, a nonlinear method like a neural net would be better able to map eye coordinates from clm trackr to screen coordinates. This intuition was correct as our estimate position is roughly .22 of the screen dimensions away from the ground truth position. Compared with the estimate from webGazer (roughly .28 away), we've achieved a 20% improvement. While this was great, we wanted to further improve by using cutouts of the eye to estimate position. In the oldRun.py file, we cutout a square surrounding the pupil center estimate from clmTrackr. From there, we feed this square into the neural network in the hopes it provides a better estimate of the gaze with more information about the eye. This fails miserably, it doesn't do significantly better than random guessing. In determining the failure of this method, we realized that we did not encode any information about the location on the eyes in the image; we only passed in the eyes. This additional bit of information proves cruical, as the eye direction alone does not allow one to determine which portion of the screen the viewer is in, and subsequently where they are staring. This led to the final iteration of the program, which can be found in run.py. This program reads the training data, extracts the cutouts of the eye, position of the pupil from clmTrackr, and ground truth from tobii and stores the data in 3 separate arrays. The data is passed to a network in the network.py file (note that the previous network iterations, netOld1 and netOld2, are also in the file). The network begins by taking the 24x24 eye cutouts and convolving them with a 5x5 filter. From there, a 4x4 average pooling with strides 4x4 is performed to generate a matrix of dimensions [batchSz, 5, 5, channelSz]. A fully connected layer maps this to a hidden layer of 16 nodes. From there, these nodes are mapped to an output of size 1 with a sigmoid activation function. Finally a weight scales this value and the pupil position from clmTrackr, adds the values, and returns this as an estimate for the gaze location. The results from the best network developed are impressive: the average distance is 0.04! This corresponds to an improvement of roughly 85%. In terms of actual pixel distance,our method has an average error of 23 pixels, while the webgazer's is 160. 
